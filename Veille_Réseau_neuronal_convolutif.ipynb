{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde6ed63-aeb3-498f-88bd-70d197ff9ee5",
   "metadata": {},
   "source": [
    "# Veille sur le Réseau neuronal convolutif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75209959-1b02-42b0-9ca5-1060723a2dd5",
   "metadata": {},
   "source": [
    "### 1. Réalisez une veille sur les réseaux de neurones artificiels de type convolutifs. Quel est l’architecture typique d’un CNN ? Quels sont ses hyperparamètres ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab00b65-d28a-4526-9e24-e107bd757d21",
   "metadata": {},
   "source": [
    "Les réseaux de neurones artificiels de type convolutifs (Convolutional Neural Networks ou CNNs) sont une classe de modèles d'apprentissage profond couramment utilisés dans le domaine de la vision par ordinateur. Ils ont révolutionné le traitement des images et sont largement utilisés dans des tâches telles que la classification d'images, la détection d'objets et la segmentation sémantique.\n",
    "\n",
    "L'architecture typique d'un CNN se compose de plusieurs couches principales :\n",
    "\n",
    "1. Couches de convolution :\n",
    "Les couches de convolution sont responsables de l'extraction des caractéristiques des images. Elles appliquent des filtres (appelés noyaux de convolution) à l'image d'entrée pour détecter des motifs tels que des bords, des textures et des formes. Chaque filtre génère une carte d'activation appelée carte de caractéristiques (feature map).\n",
    "\n",
    "2. Couches de pooling :\n",
    "Les couches de pooling sont utilisées pour réduire la dimension spatiale des cartes de caractéristiques tout en conservant les informations les plus saillantes. La méthode de pooling la plus courante est le MaxPooling, qui sélectionne la valeur maximale dans une région donnée de la carte de caractéristiques. Cela permet de réduire la taille des données et de favoriser l'invariance aux translations et aux petites variations.\n",
    "\n",
    "3. Couches entièrement connectées :\n",
    "Les couches entièrement connectées sont utilisées pour la classification finale ou la prédiction. Elles prennent les caractéristiques extraites par les couches de convolution et de pooling et les transforment en une sortie finale. Ces couches sont similaires à celles des réseaux de neurones classiques.\n",
    "\n",
    "En ce qui concerne les hyperparamètres d'un CNN, voici les principaux :\n",
    "\n",
    "1. Taille du noyau de convolution :\n",
    "La taille du noyau de convolution détermine la taille de la fenêtre qui est appliquée à l'image d'entrée pour extraire les caractéristiques locales. Une taille typique est de 3x3 ou 5x5, mais cela peut varier en fonction de la complexité de la tâche et de la taille des images.\n",
    "\n",
    "2. Nombre de filtres de convolution :\n",
    "Le nombre de filtres de convolution détermine le nombre de caractéristiques détectées à chaque couche de convolution. Un nombre plus élevé de filtres permet de capturer une plus grande diversité de caractéristiques, mais cela augmente également la complexité du modèle.\n",
    "\n",
    "3. Taille et pas du pooling :\n",
    "La taille du pooling détermine la taille de la fenêtre utilisée pour réduire les dimensions des cartes de caractéristiques. Un pas (stride) est également spécifié pour indiquer de combien de pixels la fenêtre se déplace à chaque étape de pooling.\n",
    "\n",
    "4. Fonctions d'activation :\n",
    "Les fonctions d'activation non linéaires sont utilisées pour introduire de la non-linéarité dans le modèle. Des fonctions couramment utilisées sont la fonction ReLU, la fonction sigmoid et la fonction tanh.\n",
    "\n",
    "5. Couches de dropout :\n",
    "Le dropout est une technique de régularisation utilisée pour réduire l'overfitting en désactivant aléatoirement certains neurones et leurs connexions lors de l'entraînement.\n",
    "\n",
    "Ces hyperparamètres doivent être ajustés de manière itérative et expérimentale en fonction de la tâche spécifique et des performances du modèle sur les données de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fa645-20b7-4699-b7de-d345bd588e67",
   "metadata": {},
   "source": [
    "### 2. Donnez le principe de fonctionnement d’une couche convolutive. Qu’est ce qu’un filtre de convolution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8d110-1cb2-4996-8412-4076ef62b8fb",
   "metadata": {},
   "source": [
    "Le principe de fonctionnement d'une couche convolutive dans un réseau de neurones convolutif (CNN) consiste à appliquer des filtres de convolution à une image ou à une carte de caractéristiques en utilisant une opération de convolution.\n",
    "\n",
    "Une couche convolutive est composée de plusieurs filtres de convolution, également appelés noyaux de convolution. Chaque filtre est une petite matrice de poids qui est glissée (convoluée) sur l'image ou la carte de caractéristiques en effectuant une multiplication élément par élément suivie d'une sommation. Cette opération de convolution est effectuée en déplaçant le filtre sur l'ensemble de l'image ou de la carte de caractéristiques, calculant ainsi une nouvelle carte de caractéristiques appelée carte d'activation.\n",
    "\n",
    "Le filtre de convolution est généralement de taille réduite par rapport à l'image ou à la carte de caractéristiques d'entrée. Par exemple, un filtre de convolution commun est de taille 3x3 ou 5x5. Lors de la convolution, le filtre est appliqué à chaque position de l'image ou de la carte de caractéristiques, et les multiplications et les sommes sont effectuées pour calculer la valeur d'un pixel dans la carte d'activation correspondante.\n",
    "\n",
    "Le filtre de convolution agit comme un détecteur de motifs ou de caractéristiques spécifiques dans l'image ou la carte de caractéristiques. En apprenant les poids du filtre pendant l'entraînement, le CNN peut détecter automatiquement des motifs tels que des bords, des textures, des formes ou des objets dans les données d'entrée. Les différentes combinaisons de poids dans les filtres de convolution permettent au modèle d'apprendre à extraire différentes caractéristiques à différentes échelles et orientations.\n",
    "\n",
    "En utilisant plusieurs filtres de convolution dans une couche convolutive, le CNN peut apprendre à détecter plusieurs caractéristiques simultanément. Chaque filtre génère sa propre carte d'activation, qui représente l'intensité de la caractéristique détectée dans l'image ou la carte de caractéristiques. Ces cartes d'activation sont ensuite transmises à la couche suivante pour une extraction plus approfondie des caractéristiques et une classification finale.\n",
    "\n",
    "En résumé, une couche convolutive dans un CNN applique des filtres de convolution à une image ou une carte de caractéristiques pour détecter des motifs spécifiques. Les filtres de convolution sont des matrices de poids qui sont glissées sur l'entrée en effectuant des multiplications et des sommes pour calculer une nouvelle carte d'activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972af66-c57e-4654-a807-6a120e281f04",
   "metadata": {},
   "source": [
    "### 3. Quelle est la fonction d’activation utilisée par un CNN ? Pourquoi est-elle la plus adaptée pour ce type de réseaux de neurones ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c49147-6220-4909-b0b6-3a9576b38262",
   "metadata": {},
   "source": [
    "La fonction d'activation couramment utilisée dans les couches convolutives des CNN est la fonction d'activation ReLU (Rectified Linear Unit).\n",
    "\n",
    "La fonction d'activation ReLU est définie comme suit : f(x) = max(0, x). Cela signifie que si la valeur d'entrée x est positive ou nulle, la fonction ReLU renvoie simplement cette valeur. Sinon, elle renvoie zéro. En d'autres termes, la fonction ReLU active le neurone si son entrée est positive, et le désactive si son entrée est négative.\n",
    "\n",
    "La fonction d'activation ReLU est couramment utilisée dans les CNN pour plusieurs raisons :\n",
    "\n",
    "1. Non-linéarité : Les réseaux de neurones ont besoin de fonctions d'activation non linéaires pour modéliser des relations complexes entre les données d'entrée et de sortie. La fonction ReLU introduit une non-linéarité simple mais efficace, ce qui permet aux CNN de représenter des fonctions non linéaires et d'apprendre des caractéristiques discriminantes plus complexes.\n",
    "\n",
    "2. Élimination des gradients : La fonction ReLU élimine les gradients négatifs, ce qui aide à résoudre le problème des gradients qui disparaissent (vanishing gradients) lors de la rétropropagation dans les réseaux de neurones profonds. L'élimination des gradients négatifs permet aux gradients de se propager plus efficacement et d'améliorer la convergence et la vitesse d'apprentissage.\n",
    "\n",
    "3. Efficacité de calcul : La fonction ReLU est computationnellement légère par rapport à d'autres fonctions d'activation, telles que la fonction sigmoïde ou la fonction tanh. Elle ne nécessite que des comparaisons et des opérations simples de seuil.\n",
    "\n",
    "4. Sparsité de l'activation : La fonction ReLU introduit de la sparsité dans les activations des neurones. Étant donné que les valeurs négatives sont mappées à zéro, seules les activations positives sont transmises à la couche suivante. Cela permet une représentation plus efficace des caractéristiques, en favorisant une plus grande sélectivité des neurones et en réduisant la redondance de l'information.\n",
    "\n",
    "En résumé, la fonction d'activation ReLU est largement utilisée dans les couches convolutives des CNN en raison de sa non-linéarité, de sa capacité à résoudre les problèmes de disparition des gradients, de son efficacité de calcul et de sa capacité à introduire de la sparsité. Cependant, il convient de noter qu'il existe également d'autres fonctions d'activation adaptées aux CNN, telles que la fonction Leaky ReLU, la fonction ELU (Exponential Linear Unit) et la fonction PReLU (Parametric Rectified Linear Unit), qui peuvent présenter des avantages dans certaines situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fd9d6-0182-4d2f-8588-d99014d8be78",
   "metadata": {},
   "source": [
    "### 4. Qu’est ce qu’une Feature Map ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30ba38-0340-4b66-9655-1bd53e4eab9e",
   "metadata": {},
   "source": [
    "Une Feature Map (carte de caractéristiques) dans un réseau de neurones convolutif (CNN) est une représentation bidimensionnelle des caractéristiques détectées par les filtres de convolution à une certaine couche du réseau. Chaque carte de caractéristiques correspond à une transformation locale de l'image d'entrée ou de la carte de caractéristiques précédente.\n",
    "\n",
    "Lorsque l'image ou la carte de caractéristiques est passée à travers les filtres de convolution, chaque filtre détecte des motifs spécifiques tels que des bords, des textures ou des formes dans la région de l'entrée qu'il recouvre. Chaque filtre génère une carte d'activation, qui est une représentation de l'intensité de la caractéristique détectée dans l'image ou la carte de caractéristiques.\n",
    "\n",
    "Les cartes de caractéristiques sont organisées en une pile tridimensionnelle, où la dimension de profondeur correspond au nombre de filtres de convolution appliqués à une certaine couche. Chaque carte de caractéristiques dans la pile capture une caractéristique spécifique, et leur combinaison permet de représenter des informations plus complexes et abstraites.\n",
    "\n",
    "Les cartes de caractéristiques sont transmises à travers les couches suivantes du réseau pour une extraction plus approfondie des caractéristiques et une classification finale. Les couches de pooling et les couches entièrement connectées peuvent être utilisées pour réduire la dimension des cartes de caractéristiques et les transformer en une représentation adaptée à la tâche de classification ou de prédiction.\n",
    "\n",
    "En résumé, les Feature Maps sont les représentations bidimensionnelles des caractéristiques détectées par les filtres de convolution dans un réseau de neurones convolutif. Elles fournissent une représentation riche en informations locales et hiérarchiques des données d'entrée, ce qui permet au modèle de capturer des motifs significatifs et d'apprendre des représentations discriminantes pour la tâche spécifique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127463c7-d117-4d7b-9717-e8e69beebae1",
   "metadata": {},
   "source": [
    "### 5. Donnez le principe de fonctionnement d’une couche de Pooling. Il existe différentes opérations de Pooling, citez en au moins deux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67749c8-bd5c-4bb6-9524-0b7688ccd68e",
   "metadata": {},
   "source": [
    "Le principe de fonctionnement d'une couche de Pooling dans un réseau de neurones convolutif (CNN) est de réduire la dimension spatiale des cartes de caractéristiques tout en conservant les informations les plus saillantes. Les opérations de Pooling sont appliquées après les couches de convolution pour agréger les caractéristiques locales et permettre une invariance aux translations et aux petites variations.\n",
    "\n",
    "L'idée de base d'une couche de Pooling est de subdiviser les cartes de caractéristiques en régions disjointes et de réduire la dimension de chaque région en utilisant une opération statistique, généralement le maximum (MaxPooling) ou la moyenne (AveragePooling), sur les valeurs contenues dans la région.\n",
    "\n",
    "Le fonctionnement général d'une couche de Pooling est le suivant :\n",
    "\n",
    "1. Diviser les cartes de caractéristiques en régions disjointes, généralement des fenêtres rectangulaires ou carrées, en se déplaçant avec un certain pas (stride). Par exemple, une fenêtre de taille 2x2 est souvent utilisée.\n",
    "\n",
    "2. Appliquer une opération statistique (MaxPooling ou AveragePooling) à chaque région pour obtenir une valeur agrégée. Dans le cas du MaxPooling, la valeur maximale de la région est conservée, tandis que dans le cas de l'AveragePooling, la moyenne des valeurs est calculée.\n",
    "\n",
    "3. Créer une nouvelle carte de caractéristiques en remplaçant chaque région par la valeur agrégée obtenue à l'étape précédente.\n",
    "\n",
    "Les opérations de Pooling les plus couramment utilisées sont :\n",
    "\n",
    "1. MaxPooling :\n",
    "L'opération MaxPooling sélectionne la valeur maximale de chaque région de la carte de caractéristiques. Cela permet de conserver les caractéristiques les plus fortes de chaque région et d'obtenir une représentation plus robuste face aux petites variations et aux translations. Le MaxPooling est souvent préféré car il a tendance à mieux préserver les caractéristiques distinctives.\n",
    "\n",
    "2. AveragePooling :\n",
    "L'opération AveragePooling calcule la moyenne des valeurs de chaque région de la carte de caractéristiques. Cela permet d'obtenir une représentation plus lisse et une réduction des informations redondantes. L'AveragePooling est utile lorsque la moyenne des valeurs dans une région est suffisante pour représenter la caractéristique.\n",
    "\n",
    "Il existe d'autres variantes de Pooling moins couramment utilisées, telles que le Pooling pondéré (Weighted Pooling), où les valeurs sont pondérées avant l'agrégation, ou le Pooling adaptatif (Adaptive Pooling), où la taille des régions varie en fonction des caractéristiques locales.\n",
    "\n",
    "En résumé, une couche de Pooling dans un CNN réduit la dimension spatiale des cartes de caractéristiques en appliquant une opération de MaxPooling ou d'AveragePooling sur des régions disjointes. Cela permet d'agréger les informations locales tout en conservant les caractéristiques les plus saillantes, favorisant ainsi l'invariance aux translations et aux petites variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace94c8e-5bec-45f9-a3ad-e875c826e38b",
   "metadata": {},
   "source": [
    "### 6. La dernière couche d’un CNN est une couche entièrement connectée. Expliquez son fonctionnement. Qu’est ce que reçoit la couche entièrement connectée ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cb87d-c902-4b1a-8143-b755b2d47585",
   "metadata": {},
   "source": [
    "La dernière couche d'un CNN est généralement une couche entièrement connectée, également appelée couche de sortie. Cette couche est responsable de la classification ou de la prédiction finale du modèle. Contrairement aux couches de convolution et de pooling qui conservent la structure spatiale des caractéristiques, la couche entièrement connectée considère les entrées comme un vecteur unidimensionnel.\n",
    "\n",
    "La couche entièrement connectée reçoit en entrée les caractéristiques extraites par les couches précédentes du CNN, qui peuvent être des cartes de caractéristiques provenant de la dernière couche de pooling. Les dimensions de ces caractéristiques dépendent de l'architecture spécifique du CNN utilisé.\n",
    "\n",
    "Pour traiter les caractéristiques en tant que vecteur unidimensionnel, les activations de chaque neurone dans les cartes de caractéristiques sont aplaties pour créer un vecteur d'entrée pour la couche entièrement connectée. Cela signifie que chaque élément dans le vecteur d'entrée correspond à une activation spécifique d'un neurone.\n",
    "\n",
    "Dans la couche entièrement connectée, chaque neurone est connecté à tous les neurones de la couche précédente. Ces connexions sont représentées par des poids qui sont appris lors de l'entraînement du modèle. Chaque neurone effectue une combinaison linéaire des entrées pondérées par ces poids, suivi de l'application d'une fonction d'activation non linéaire pour générer une sortie.\n",
    "\n",
    "La fonction d'activation utilisée dans la couche entièrement connectée peut varier en fonction de la tâche spécifique. Par exemple, pour la classification binaire, une fonction d'activation couramment utilisée est la fonction sigmoïde, qui produit une sortie dans l'intervalle [0, 1] représentant la probabilité d'appartenance à la classe positive. Pour la classification multiclasse, une fonction d'activation telle que la fonction softmax est utilisée pour normaliser les sorties en probabilités pour chaque classe.\n",
    "\n",
    "En résumé, la dernière couche entièrement connectée d'un CNN reçoit les caractéristiques extraites par les couches précédentes du modèle en tant que vecteur unidimensionnel. Chaque neurone dans cette couche est connecté à tous les neurones de la couche précédente et génère une sortie en effectuant une combinaison linéaire pondérée suivie d'une fonction d'activation. Cette couche est responsable de la classification ou de la prédiction finale du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fed74-a576-47fc-afea-b3b30a8651c2",
   "metadata": {},
   "source": [
    "### 7. Pour quelles raisons un réseau de neurones convolutif est-il préféré à un réseau de neurones dense pour une tâche de classification d'images ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a7aa6-32ae-43c0-a4cf-2780b512358d",
   "metadata": {},
   "source": [
    "Un réseau de neurones convolutif (CNN) est généralement préféré à un réseau de neurones dense pour une tâche de classification d'images en raison de plusieurs raisons :\n",
    "\n",
    "1. Exploitation de la structure spatiale : Les CNN sont conçus pour exploiter la structure spatiale inhérente aux images. Ils utilisent des couches de convolution pour détecter des motifs locaux et des caractéristiques dans différentes régions de l'image. Cela permet aux CNN de capturer les relations spatiales entre les pixels et les motifs dans l'image, ce qui est crucial pour la classification d'images.\n",
    "\n",
    "2. Réduction du nombre de paramètres : Les CNN réduisent considérablement le nombre de paramètres par rapport aux réseaux de neurones denses traditionnels. Au lieu de connecter chaque neurone de la couche précédente à chaque neurone de la couche suivante, les filtres de convolution partagent leurs poids spatialement. Cela permet de réduire la complexité du modèle, de diminuer les besoins en mémoire et de faciliter l'entraînement sur de grandes bases de données d'images.\n",
    "\n",
    "3. Translation invariance : Les CNN sont capables d'apprendre des caractéristiques invariantes aux translations. Les filtres de convolution appliqués à différentes régions de l'image partagent les mêmes poids, ce qui permet aux CNN de reconnaître les mêmes motifs ou caractéristiques peu importe leur position dans l'image. Cela permet aux CNN de généraliser efficacement à des images similaires même si elles sont légèrement décalées ou présentent des variations de position.\n",
    "\n",
    "4. Gestion des données de grande taille : Les CNN sont conçus pour gérer des données de grande taille, telles que des images haute résolution. Les opérations de convolution et de pooling permettent de réduire la taille spatiale des caractéristiques tout en conservant les informations les plus importantes. Cela permet de réduire la complexité de calcul et les besoins en mémoire lors du traitement de grandes images.\n",
    "\n",
    "5. Capacité à apprendre des hiérarchies de caractéristiques : Les CNN sont capables d'apprendre des hiérarchies de caractéristiques à différents niveaux d'abstraction. Les couches de convolution successives extraient des caractéristiques de plus en plus complexes, en combinant et en agglomérant les caractéristiques de bas niveau pour former des représentations de plus haut niveau. Cela permet aux CNN de capturer des informations discriminantes à différents niveaux de détail et de réaliser une classification plus précise.\n",
    "\n",
    "En résumé, les CNN sont préférés aux réseaux de neurones denses pour la classification d'images en raison de leur capacité à exploiter la structure spatiale des images, de leur réduction du nombre de paramètres, de leur invariance aux translations, de leur capacité à gérer des données de grande taille et de leur capacité à apprendre des hiérarchies de caractéristiques. Ces avantages font des CNN des modèles très efficaces pour les tâches de classification d'images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
